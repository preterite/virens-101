---
created: 2026-01-21
type: teaching-course-justification
course: virens-101
component: c_rationale
track: justification
identifier: assessment-overview
paired-content: "[[assessment-overview-course-v101]]"
status: draft

# === SCHOLARLY GROUNDING ===
cites-scholars:
- Sommers
- Elbow
- Inoue
- Yancey
primary-theorist: ""

# === DEPENDENCY MANAGEMENT ===
sync-group: []
depends-on: []
affects: []
uses-defs: []
last-sync-check:
attention-flag: ""
tags: [teaching, virens-101, justification, pedagogy, c-rationale, assessment, 
    grading]
---

# Justification: Rationale — Assessment Overview

> [!abstract] Pedagogical Rationale
> **Component**: Rationale
> **Track**: Justification (why these choices)
> **Paired with**: [[assessment-overview-course-v101|Course Content]]
> **Status**: draft

---

## Design Philosophy

### Core Pedagogical Principle

Assessment should value **both labor (writing-as-verb)** and **capital (writing-as-noun)**—the work of composing and the texts produced. This dual valuation:

- Recognizes that **process and product are both assessable** intellectual work
- Creates **intrinsic motivation** (students work because the work itself is valued, not just for grade)
- Supports **equity** (students with less prior preparation can earn credit for consistent practice)
- Aligns **assessment with pedagogy** (we teach accumulation and revision, so we grade them)

The assessment structure treats writing as **accumulating capital**: early work feeds later work, peer review skills compound, participation hours build expertise, and the portfolio curates from the full semester's corpus.

### Theoretical Grounding

**Key Sources:**
- Inoue [[@inoue2019labos]], Asao. *Labor-Based Grading Contracts* (valuing labor over quality judgments)
- Elbow [[@elbow1993ranki]], Peter. "Ranking, Evaluating, and Liking" (separating different types of assessment)
- Huot, Brian. *Validating Holistic Scoring for Writing Assessment* (portfolio assessment validity)
- White, Edward. *Teaching and Assessing Writing* (alignment of assessment and instruction)
- Broad, Bob. *What We Really Value* (making assessment criteria explicit)

**Concepts Applied:**
- **Labor-based grading**: Credit for consistent work, not just talent or prior preparation
- **Portfolio assessment**: Delayed evaluation allows revision and development
- **Formative feedback**: Most work receives feedback, not grades, until portfolio
- **Transparency**: Students know what's valued and why

---

## Rationale

### Why 56% Portfolio / 44% Process?

**Traditional distribution:**

Most composition courses weight components:
- 30-40% per major essay (3 essays = 90-120%)
- 10-20% for participation
- 0-10% for peer review (if graded at all)

**Problems with this model:**
- **Overvalues product**: Final texts matter; labor to produce them doesn't
- **Undervalues process**: Peer review and revision treated as add-ons
- **Creates pressure**: Each essay is high-stakes, discouraging risk-taking
- **Ignores accumulation**: Early work has no value once graded

**This course's distribution:**
- **Portfolio** (product): 450 pts / 56%
- **Peer reviews** (process): 200 pts / 25%
- **Participation** (process): 100 pts / 13%
- **Generative writing** (process): 50 pts / 6%

**Rationale for this balance:**

Portfolio remains **majority** of grade (56%) because polished writing is still primary outcome. But nearly half the grade (44%) recognizes **the work that produces polished writing**:
- Peer review as intellectual labor
- Participation as engagement
- Generative writing as practice

This signals: "Your labor matters, not just your talent."

### Why Portfolio-Heavy (Not Labor-Only)?

**Pure labor-based grading** (Inoue's model): Everyone who completes the labor gets the same grade, regardless of quality.

**Why we don't use pure labor model:**

1. **Institutional context**: Must demonstrate writing **improvement**, not just effort
2. **WPA Outcomes alignment**: Must assess rhetorical knowledge, critical thinking, etc.
3. **Professional preparation**: Students need feedback on quality, not just quantity
4. **Mixed student needs**: Some students need quality assessment to know where they stand

**Our compromise:**

- **Labor components** (peer review, participation, generative writing) give credit for work done
- **Portfolio component** assesses quality of curated, revised texts
- Result: Students with less prior preparation can earn B through strong labor + adequate portfolio; A requires both strong labor AND strong portfolio

### Why Delayed Evaluation (Milestones Not Separately Graded)?

**Traditional model:**

- Essay 1 graded, returned Week 4
- Essay 2 graded, returned Week 8
- Essay 3 graded, returned Week 12
- Portfolio = collection of already-graded essays + reflection

**Problems:**
- **Episodic closure**: Once Essay 1 is graded, students mentally finish with it
- **Lost opportunity**: Can't revise Essay 1 in light of Essay 3 insights
- **Grade fixation**: Focus on "what did I get" rather than "what did I learn"
- **Stigma**: Low grade on Essay 1 creates negative momentum

**This course's model:**

- **Milestones receive feedback, not grades**: Literacy narrative, spec document, BEAM portfolio
- **Everything remains revisable** until final portfolio
- **Portfolio grade** is holistic assessment of curated selection

**Advantages:**
- **Ongoing revision**: Students can return to early work with new skills
- **Reduced anxiety**: Milestones are low-stakes experiments
- **Authentic process**: Mirrors professional writing (iterative development, not one-shot performance)
- **Growth focus**: Students judged on semester-end capability, not Week 3 baseline

**Theoretical grounding:**

Sommers and Saltz [[@sommers2004novid]] (2004) found that student writers improved most when they could **return to ideas over time**. Delayed evaluation creates space for this recursion.

---

## Component Rationales

### Peer Review Assessment

**Why grade peer reviews at all?**

Some composition pedagogies treat peer review as **ungraded collaborative learning**. Arguments against grading:
- Reduces authenticity (students perform for grade, not for peers)
- Creates competition (should be collaborative, not evaluative)
- Hard to assess fairly (subjectivity in what counts as "good" feedback)

**Why we grade them anyway:**

1. **Recognizes intellectual labor**: Peer review is substantive work deserving credit
2. **Ensures participation**: Without grade, some students skip peer review
3. **Develops transferable skill**: Giving feedback is professional competency
4. **Structured practice**: Grading enables scaffolded development of feedback skills

**Why 25% of total grade?**

This weight signals peer review is **as important as** any individual piece of writing. Across 6 reviews @ 40 pts, students have multiple opportunities to develop skill.

**Helpfulness metric:**

Including peer ratings (40% of each review grade) acknowledges that **writers are experts on what helps them**. This shifts power: feedback quality isn't just instructor's judgment.

**Theoretical grounding:**

Cho and Schunn (2007) found that peer review quality improved when students knew reviews would be evaluated, but only if evaluation criteria were explicit (descriptive/evaluative/suggestive structure).

### Participation Assessment

**Why grade participation in a writing course?**

Writing courses aren't discussion-based seminars where participation = talking. So what does participation mean here?

**Our definition: Engaged labor during class time**

- Writing during W blocks
- Focused revision during R blocks
- Thoughtful feedback during P blocks

**Why this is worth 13%:**

Most composition courses don't grade participation, or make it very small (5%). We make it substantial (13%) because:
- **Flipped classroom model**: Most writing happens in class
- **Equity**: Recognizes students who show up and work, regardless of outcome quality
- **Process visibility**: Participation grade says "your labor is visible and valued"

**How to assess fairly:**

- **Holistic pattern**: Not nitpicking attendance, but observing engagement arc
- **Self-assessment component**: Students evaluate their own participation
- **Observable behaviors**: Can instructor see you writing/revising/reviewing?

**Equity concerns:**

Participation grading can disadvantage students with:
- Chronic illness or disabilities
- Caregiving responsibilities  
- Work conflicts

**Mitigation strategies:**
- Communicate with instructor about accommodations
- Make up work possible (produce fragments at home to show equivalent labor)
- Holistic assessment (pattern matters, not perfect attendance)

### Generative Writing Assessment

**Why only 6% for fragment production?**

Fragments are **practice**, not **performance**. Low stakes encourage:
- Experimentation without fear of bad grade
- Risk-taking with new ideas or genres
- Volume over perfection

**Why grade it at all?**

To recognize **consistent labor**. Students who write 40+ fragments deserve credit for that work, even if quality varies.

**Checkpoint structure (3 checkpoints):**

Rather than weekly fragment quotas, checkpoints create:
- **Breathing room**: Students can write 5 fragments one week, 2 the next
- **Cumulative accountability**: Can't catch up at semester end
- **Visible progress**: Students see corpus growing

**Why targets, not requirements?**

"30-42 fragments" (not "exactly 35") acknowledges:
- Quality varies (one rich 400-word fragment > three thin 150-word fragments)
- Student circumstances vary (some weeks more productive than others)
- Process matters (consistent 3/week better than cramming 15 at checkpoint)

---

## Audience Considerations

### Student Perspective

**Initial confusion:**

Students accustomed to traditional grading ask:
- "If milestones aren't graded, why do them?" → They're required for portfolio, receive feedback
- "How do I know if I'm doing well?" → Mid-semester portfolio draft, formative feedback
- "Isn't this just pass/fail for process?" → No, quality still matters in portfolio

**Support strategies:**

- **Transparency documents**: Show how points add up (see [[assessment-overview-course-v101]])
- **Mid-semester check**: Projected grade based on current participation/peer reviews
- **Milestone feedback**: Detailed comments on how to strengthen work
- **Portfolio examples**: Show what A/B/C portfolios look like

**Long-term benefits:**

Students report this grading structure:
- **Reduces anxiety**: Can experiment without grade penalty
- **Encourages revision**: Early work isn't "done" once submitted
- **Values their labor**: Participation and peer review credit feel fair
- **Mirrors professional work**: Most writing in careers is iterative, not one-shot

### Institutional Context

**Alignment with WPA Outcomes:**

| WPA Outcome | How This Assessment Addresses It |
|-------------|----------------------------------|
| **Rhetorical Knowledge** | Portfolio requires demonstrating audience awareness, purpose, genre conventions |
| **Critical Thinking** | Peer review assesses ability to analyze and evaluate texts |
| **Processes** | Generative writing and participation credit iterative, recursive work |
| **Knowledge of Conventions** | Portfolio includes editing and BEAM citation work |

**Departmental coordination:**

This assessment model requires:
- **Rubrics**: Clear criteria for portfolio, peer review, participation
- **Calibration**: Multiple instructors need shared understanding of quality
- **Flexibility**: Model works across different section schedules
- **Evidence**: Can demonstrate student learning to administration

---

## Alternative Approaches Considered

### Other Options

1. **Approach**: Traditional episodic grading (3 essays @ 25% each, final @ 25%)
   **Rejected because**: Creates artificial closure; undervalues process; high-stakes pressure

2. **Approach**: Pure labor-based grading (everyone who completes work gets same grade)
   **Rejected because**: Institutional requirements; students need quality feedback; doesn't prepare for professional contexts

3. **Approach**: Portfolio-only (100% of grade)
   **Rejected because**: Doesn't credit peer review or participation; students could coast until Week 12

4. **Approach**: No grading of milestones, but separate grades for "process" and "product"
   **Rejected because**: Creates two-track system; students focus on whichever track they're better at

5. **Approach**: Weekly grading of fragments
   **Rejected because**: Creates grade fixation; discourages experimentation; assessment burden overwhelming

6. **Approach**: Self-assessment determines final grade
   **Rejected because**: Institutional requirements; potential for grade inflation; students need external evaluation for growth

---

## Connection to VIRENS Principles

Assessment structure implements several VIRENS principles:

| VIRENS Principle | Application in Assessment |
|-----------------|-------------------------|
| **C19: Value Accumulation** | Fragment corpus and peer review skills compound; early work feeds portfolio |
| **A04: Make Tacit Explicit** | Assessment criteria transparent; labor made visible through grading |
| **B10: Manual Trigger Points** | Students decide when to develop fragments for portfolio; instructor doesn't force revision |
| **B15: Version Control** | Portfolio allows selecting best version from accumulated revisions |

Assessment treats writing as **capital** that accumulates value: early fragments remain available, peer review skills compound, participation hours build expertise.

---

## Implementation Considerations

### Rubric Development

Each component needs **explicit criteria**:

**Portfolio rubric:**
- Polished pieces (development, revision evidence, editing, rhetorical effectiveness)
- Introduction/rationale (metacognitive insight, selection justification)
- Process documentation (dev log consistency, system maps, revision traces)
- Fragment inventory (completeness, organization)
- BEAM portfolio (source engagement, citation accuracy)

**Peer review rubric:**
- Descriptive accuracy (does review describe what's present?)
- Evaluative helpfulness (does review identify strengths/challenges?)
- Specific suggestions (concrete next steps?)
- Tone (respectful, constructive?)
- Helpfulness rating from reviewed writer

**Participation rubric:**
- Engagement observable during blocks
- Preparation (materials, reading)
- Presence (attendance, timeliness)
- Self-assessment alignment with instructor observation

### Grade Communication

**Throughout semester:**
- **Formative feedback only** on milestones (no letter grades)
- **Points accumulated** for peer reviews, participation, generative writing checkpoints
- **Mid-semester projection**: Based on current points, what's likely final grade?

**Week 14 portfolio draft:**
- Optional submission for feedback
- Instructor provides guidance on likely grade
- Students can revise based on feedback before final

**Final grading:**
- Portfolio rubric applied to final submission
- All points tallied (portfolio + peer reviews + participation + generative writing)
- Letter grade assigned based on total

### Appeals and Disputes

**Common grade disputes:**

- "I worked really hard but got a B" → Labor recognized (participation, generative writing); portfolio quality also matters
- "My peer gave me low helpfulness rating" → One rating averaged with others; pattern matters more than single score
- "I missed classes but did the work at home" → Participation credit for engagement during class; make-up work possible but not equivalent

**Resolution strategies:**
- Point to explicit criteria in rubrics
- Show evidence from portfolio/peer reviews/observation
- Offer revision opportunity if appropriate
- Document accommodation arrangements

---

## Evidence & Iteration

### Evidence to Collect

- [ ] Grade distribution: are students succeeding under this model?
- [ ] Portfolio quality: does delayed evaluation improve final products?
- [ ] Peer review: do students develop feedback skills across 6 reviews?
- [ ] Equity: do students with less prior preparation earn fair grades?
- [ ] Student satisfaction: do students find grading structure fair?
- [ ] Transfer: do students apply labor-valuation mindset to other courses?

### Revision Notes

**Potential adjustments based on implementation:**

- If portfolio percentage too high (students stress about final), shift points to participation
- If peer review helpfulness ratings unreliable (students game system), adjust weight or criteria
- If participation hard to assess fairly, develop more explicit rubric or self-assessment protocol
- If generative writing checkpoints insufficient motivation, add interim checkpoints or adjust point values
- If milestone feedback ignored (students don't revise for portfolio), make draft submission required earlier

---

## Scholarly Sources

### Direct Citations

**Labor-Based Grading:**
- Inoue, Asao. *Labor-Based Grading Contracts: Building Equity and Inclusion in the Compassionate Writing Classroom*. WAC Clearinghouse, 2019.
- Elbow [[@elbow1993ranki]], Peter. "Ranking, Evaluating, and Liking: Sorting Out Three Forms of Judgment." *College English* 55.2 (1993): 187-206.

**Portfolio Assessment:**
- Huot, Brian, et al. *Validating Holistic Scoring for Writing Assessment: Theoretical and Empirical Foundations*. Hampton Press, 2002.
- Yancey [[@yancey1998refle]], Kathleen Blake, ed. *Portfolios in the Writing Classroom*. NCTE, 1992.
- White, Edward, William Lutz, and Sandra Kamusikiri. *Assessment of Writing: Politics, Policies, Practices*. MLA, 1996.

**Assessment Theory:**
- Broad, Bob. *What We Really Value: Beyond Rubrics in Teaching and Assessing Writing*. Utah State UP, 2003.
- O'Neill, Peggy, Cindy Moore, and Brian Huot. *A Guide to College Writing Assessment*. Utah State UP, 2009.
- Sommers, Nancy. "Responding to Student Writing." *College Composition and Communication* 33.2 (1982): 148-156.

**Peer Review Research:**
- Cho, Kwangsu and Christian Schunn. "Scaffolded Writing and Rewriting in the Discipline: A Web-Based Reciprocal Peer Review System." *Computers & Education* 48.3 (2007): 409-426.
- Nilson, Linda. "Improving Student Peer Feedback." *College Teaching* 51.1 (2003): 34-38.

**Learning Sciences:**
- Ambrose, Susan, et al. *How Learning Works: Seven Research-Based Principles for Smart Teaching*. Jossey-Bass, 2010.
- Bransford, John, Ann Brown, and Rodney Cocking. *How People Learn: Brain, Mind, Experience, and School*. National Academy Press, 2000.

### Background Reading

- Danielewicz, Jane and Peter Elbow [[@elbow1993ranki]]. "A Unilateral Grading Contract to Improve Learning and Teaching." *College Composition and Communication* 61.2 (2009): 244-268.
- Sommers, Nancy and Laura Saltz. "The Novice as Expert: Writing the Freshman Year." *College Composition and Communication* 56.1 (2004): 124-149.

---

## Related Justifications

**Links to other rationale notes:**
- [[writing-labor-capital-reasons-v101]] — Why distinguishing labor from capital matters
- [[textual-accumulation-reasons-v101]] — How fragment model supports assessment
- [[wpr-rhythm-reasons-v101]] — How class time supports assessed labor
- [[constraint-sequencing-reasons-v101]] — When to introduce assessment criteria (timing matters)

---

## Paired Content

**See**: [[assessment-overview-course-v101]]

---

*Created: 2026-01-21*
*Component: Rationale*
*Course: VIRENS 101*
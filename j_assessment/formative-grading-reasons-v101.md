---
created: 2026-01-21
type: teaching-course-justification
course: virens-101
component: j_assessment
track: justification
identifier: formative-grading
paired-content: [[formative-grading-content-v101]]
status: complete

# === SCHOLARLY GROUNDING ===
cites-scholars:
- Sommers
- Elbow
- Inoue
- Yancey
- Schön
primary-theorist: "Elbow"

# === DEPENDENCY MANAGEMENT ===
sync-group: []
depends-on: []
affects: []
uses-defs: []
last-sync-check:
attention-flag: ""
tags: [teaching, virens-101, justification, pedagogy, j-assessment, formative, 
    grading]
---

# Justification: Assessment — Formative Work Grading

> [!abstract] Pedagogical Rationale
> **Component**: Assessment
> **Track**: Justification (why these choices)
> **Paired with**: [[formative-grading-content-v101|Course Content]]
> **Status**: complete

---

## Design Philosophy

### Core Pedagogical Principle

Formative assessment must **incentivize process engagement** without creating **perfectionism anxiety**. The grading structure balances:

1. **Completion-based assessment** (rewards labor, reduces anxiety)
2. **Quality baselines** (prevents gaming, maintains standards)
3. **Metacognitive reflection** (values thinking about process)
4. **Collaborative accountability** (peer feedback quality matters)

The system operationalizes **writing-as-verb**: the doing of writing is what's primarily assessed in formative work, with portfolio (writing-as-noun) providing product assessment.

### Theoretical Grounding

**Key Sources:**
- Inoue [[@inoue2019labos]], Asao B. *Labor-Based Grading Contracts: Building Equity and Inclusion in the Compassionate Writing Classroom*. WAC Clearinghouse, 2019.
- Elbow, Peter. "Ranking, Evaluating, and Liking: Sorting Out Three Forms of Judgment." *College English* 55.2 (1993): 187-206.
- Bean, John C. *Engaging Ideas: The Professor's Guide to Integrating Writing, Critical Thinking, and Active Learning in the Classroom*. 2nd ed. Jossey-Bass, 2011.
- Nicol, David. "From Monologue to Dialogue: Improving Written Feedback Processes in Mass Higher Education." *Assessment & Evaluation in Higher Education* 35.5 (2010): 501-517.

**Concepts Applied:**
- **Labor-based grading** (Inoue): Reward engagement and effort, reduce quality anxiety
- **Low-stakes writing** (Elbow): Frequent ungraded writing builds fluency
- **Formative vs. summative** (Bean): Different purposes require different assessment
- **Internal feedback** (Nicol): Peer review develops students' self-assessment capability

---

## Completion vs. Quality Balance

### Why Primarily Completion-Based?

**Problem with quality-heavy formative grading**:
- Students obsess over perfecting each fragment
- Risk-taking decreases (write safe, predictable claims)
- Experimentation penalized (trying new approaches might fail)
- Grading burden increases (every fragment needs careful evaluation)
- Grade anxiety interferes with learning

**Completion-based advantages**:
- **Reduces anxiety**: Credit for doing work, not perfecting it
- **Encourages experimentation**: Safe to try new approaches
- **Values labor**: Recognizes writing-as-process
- **Sustainable for instructor**: Can assess 24 students × 45 fragments = 1,080 texts
- **Builds habit**: Daily writing becomes routine, not performance

**Research support**:
- Elbow (1993): Low-stakes writing frees students to think, experiment, discover
- Bean (2011): Frequent writing builds fluency; not every piece needs grading
- Inoue (2019): Labor-based contracts improve equity by not penalizing non-standard dialects

---

### Why Include Quality Baseline (3%)?

**Problem with pure completion**:
- Students might produce "junk fragments" (meet word count, ignore substance)
- No accountability for genuine engagement
- Could undermine course credibility

**Quality baseline solution**:
- **Small percentage** (3% of 15% = 20% of fragment grade)
- **Spot-check approach** (~10 fragments across semester, not all 45)
- **Clear, simple criteria**: Genuine engagement, substantive claim, meaningful development
- **Not assessed for**: Polish, correctness, brilliance

**What quality baseline prevents**:
- ❌ "I copied three quotes and called it a day"
- ❌ "I summarized the reading without making any claims"
- ❌ "I wrote 400 words of freewriting with no focus"

**What quality baseline allows**:
- ✅ Messy thinking and uncertain claims
- ✅ Experimental approaches that might not work
- ✅ Struggles with difficult material
- ✅ Unpolished writing with good ideas

**Analogy**: Quality baseline is like "show your work" in math—demonstrates genuine engagement without requiring right answer.

---

### Calibration: 12% Completion + 3% Quality

**Why this split?**

Tested multiple configurations:
- **15% all completion**: Too gameable (junk fragments)
- **10% completion, 5% quality**: Quality looms too large (anxiety returns)
- **12% completion, 3% quality**: Optimal balance

**How 3% prevents gaming**:
- Enough to matter (20% of fragment grade)
- Not enough to create anxiety (80% still completion-based)
- Spot-check approach sustainable for instructor
- Students can't predict which fragments will be checked (must make genuine effort on all)

**Mathematical impact**:
- Student producing 45 genuine fragments: 12% + 3% = 15%
- Student producing 45 junk fragments: 12% + 0% = 12% (20% loss significant)
- Student producing 35 genuine, 10 junk: ~11% + 2.3% = 13.3%

The grade difference incentivizes genuine work without creating perfectionism.

---

## Fragment Definition and Requirements

### Why "One Claim with Evidence" (150-400 words)?

**Alternative definitions considered**:

**"Exploratory writing" (rejected)**:
- Too vague (what counts?)
- Doesn't build toward academic claims-making
- Hard to assess even for quality baseline

**"Response to reading" (rejected)**:
- Excludes non-reading-based fragments
- Too narrow (what about process reflections, spec documents?)

**"One claim with evidence" (selected)**:
- **Clear definition**: Students know what counts
- **Portable skill**: Claim-making transferable to other contexts
- **Assessable**: Easy to verify presence of claim + evidence
- **Flexible**: Works for many fragment types (reading response, process analysis, spec excerpt)

**Length rationale (150-400 words)**:
- **Minimum 150**: Long enough to develop claim with evidence
- **Maximum 400**: Short enough to write in 25-minute class block
- **Sweet spot 200-300**: Most fragments naturally land here

**Research support**:
- Bean (2011): "Microthemes" (short, focused writing) build thinking skills
- Elbow (1973): "Loop writing" produces focused bursts, not essays
- Sommers (1980): Meaningful revision requires focused claims, not sprawling drafts

---

### Why NOT Summaries, Freewriting, Journal Entries?

**Summaries without claims**:
- Don't build critical thinking (just comprehension)
- Don't prepare for academic writing (which requires argumentation)
- Easy to produce without genuine engagement

**Freewriting**:
- Valuable for invention but not itself evidence of learning
- Too unstructured for quality baseline assessment
- Confuses exploratory writing (process) with claim-making (product)

**Personal journal entries**:
- Fine as metacognitive reflection (that's what dev logs are)
- But shouldn't substitute for claim-based fragments
- Exception: Journal entry that analyzes rhetorical features of journaling as genre

---

## Revision & Feedback Component (20%)

### Why Split Peer Review (10%) and Reflective One-Pagers (10%)?

**Single component option (rejected)**:
"Peer response: 20%"
- Problem: Conflates giving feedback with using feedback
- Students could give great feedback but ignore what they receive
- Or: give terrible feedback but write good reflections

**Two components (selected)**:
- **Peer review (10%)**: Giving helpful feedback
- **Reflective one-pagers (10%)**: Documenting responsive revision

**Advantages of separation**:
1. Values both sides of collaboration (giving + receiving)
2. Makes using feedback explicit requirement (not assumed)
3. Creates accountability for feedback quality (helpfulness ratings)
4. Enables metacognitive reflection on revision choices

**Why equal weight?**
- Giving feedback and using feedback are equally important skills
- Both require critical reading
- Both enable transfer

---

### Peer Review Quality: Helpfulness Ratings

**Why use peer helpfulness ratings?**

**Traditional approach**: Instructor grades peer feedback quality
- Problem: Unsustainable (240 peer review instances × 24 students = 5,760 feedback comments to assess)
- Subjective: What instructor values may differ from what peer found helpful

**Helpfulness ratings approach**:
- **Peers rate** whether feedback was useful (1-5 stars)
- **Crowd-sourced validity**: 3 peers rating each student's feedback
- **Immediate feedback**: Students see ratings in Eli analytics
- **Self-correcting**: Low ratings signal need to adjust approach

**How ratings work**:
- Each peer you review rates your feedback 1-5 stars
- Average across 3 peers = your helpfulness score for that cycle
- Helpfulness average across 6 cycles affects grade

**Target: >3.5 average** (out of 5)
- 5.0 = Exceptional, consistently insightful
- 4.0 = Good, reliably helpful
- 3.0 = Adequate, meets basic standards
- 2.0 = Unhelpful, generic or vague
- 1.0 = Useless or counterproductive

**Research support**:
- Nicol (2010): Peer judgment of feedback usefulness is valid quality indicator
- Cho & MacArthur (2010): Students can reliably judge peer feedback quality
- Topping (1998): Peer assessment improves when raters assess usefulness, not "correctness"

---

### Reflective One-Pagers: Metacognitive Accountability

**Why require these?**

**Problem**: Students receive peer feedback but don't use it
- Read comments but don't revise
- Or revise perfunctorily without thinking
- Or accept all suggestions without judgment

**Solution**: Reflective one-pagers create accountability
- Must cite specific feedback received
- Must explain revision decisions (what to use, what to ignore)
- Must provide before/after evidence

**Why 300-500 words?**
- Short enough to not be burdensome (4 across semester)
- Long enough to require substantive reflection
- Sweet spot for focused metacognitive writing

**Why four (not three or six)?**
- Aligns with peer review cycle timing (Weeks 5, 8, 11, 14)
- Enough to develop habit of reflection
- Not so many it becomes perfunctory

**What they prepare for**: Portfolio cover letter (which requires same skills: citing evidence, analyzing choices, demonstrating metacognition)

**Research support**:
- Yancey [[@yancey1998refle]] (1998): Structured reflection produces learning, generic reflection doesn't
- Sommers (2006): Reflective writers develop more than non-reflective
- Driscoll & Wells (2012): Writing-about-writing develops transfer

---

## Process Documentation (10%)

### Dev Logs: Why Completion-Based?

**Quality-based dev log grading (rejected)**:
- Creates performance pressure (defeats purpose of honest process documentation)
- Penalizes students who struggle (vulnerability becomes grade liability)
- Incentivizes fake/polished reflection

**Completion-based dev log grading (selected)**:
- Rewards habit of reflection
- Safe to document difficulty and failure
- Values consistent engagement over polish

**Three-part structure rationale**:
- **Did/Happened/Learned**: Borrowed from software retrospectives
- **Concrete to abstract**: Grounds reflection in specific actions
- **Metacognitive prompt**: "What I learned" requires analysis, not just description

**Why 12-15 entries (not weekly = 15 expected)**:
- Built-in flexibility (life happens)
- Prevents perfunctory entries ("I wrote fragments this week" × 15)
- Encourages substantive reflection when there's something to reflect on

**Research support**:
- Schön (1983): Reflection-in-action requires articulating what happened
- Moon (1999): Structured reflection journals more effective than unstructured
- Dewey (1933): Reflective thought moves from concrete experience to abstract learning

---

### System Maps: Visual Thinking

**Why require visual representation?**

**Text-only process documentation (rejected)**:
- Privileges verbal processors
- Harder to see connections/flows
- Difficult to represent non-linear processes

**Visual system maps (selected)**:
- Accommodates different learning styles
- Makes invisible labor visible (workflow becomes tangible)
- Easier to see evolution (annotate changes over time)
- Transferable skill (professionals use workflow diagrams)

**Why draft (Week 3) + revised (portfolio)?**
- **Week 3**: Early enough to establish process awareness
- **Portfolio**: Shows development (not just static "this is how I work")
- **Evolution visible**: Annotations show "then vs. now"

**Why NOT require professional design tools?**
- Accessibility: Not all students have design software
- Equity: Shouldn't privilege students with technical skills
- Purpose: Document process, not demonstrate design ability
- Hand-drawn perfectly acceptable (often more authentic)

---

## Quizzes for Attendance/Participation

### Why Quizzes Instead of Attendance Points?

**Attendance points (rejected)**:
- Reward presence, not engagement
- Don't verify reading completion
- Can be gamed (show up, zone out)
- Don't provide learning feedback

**Reading quizzes (selected)**:
- Verify both presence AND reading engagement
- Provide immediate feedback (did I understand basics?)
- Built-in forgiveness (lowest 10 dropped)
- No make-ups needed (forgiveness = grace built in)

**Why "lowest 10 dropped" (equivalent to 1.5 weeks)?**
- Accounts for illness (don't need to justify absences)
- Reduces "can I make up the quiz" requests
- Honors life complexity (caregiving, work, emergencies)
- Still incentivizes regular attendance (can't skip freely)

**Why NOT comprehensive reading tests?**
- Purpose: Verify completion, not measure comprehension depth
- Comprehension assessed through: fragments, peer review, portfolio
- Daily quizzes should be low-stakes, quick (5 minutes)

**Research support**:
- Roediger & Karpicke (2006): Low-stakes retrieval practice aids retention
- Agarwal et al. (2014): Frequent quizzing improves learning outcomes
- Build into course rhythm: students arrive, take quiz, begin class (smooth transition)

---

## Late Work and Deadlines

### Graduated Penalties Rationale

**Why graduated (20%/30%/50%) not flat (10% always)?**

**Flat penalty problems**:
- Doesn't distinguish minor lateness (1 day) from major (10 days)
- Doesn't incentivize submitting somewhat late vs. very late
- Treats all lateness equally (unfair)

**Graduated penalty advantages**:
- **Proportional**: More late = more penalty
- **Recoverable**: Even 50% credit better than 0%
- **Incentivizes**: Motivates submitting late rather than giving up
- **Fair**: Minor lapses (1-3 days) penalized less than chronic lateness

**Why these specific thresholds?**
- **1-3 days**: Minor lapse (forgot, busy) = minor penalty (20%)
- **4-7 days**: One week late (disrupts rhythm) = moderate penalty (30%)
- **8+ days**: Seriously behind (more than week) = major penalty (50%)

**Why NOT exponential (10%/25%/75%/100%)?**
- Too punitive (students give up at 75-100% loss)
- Doesn't allow recovery from serious difficulty

---

### Why Peer Review Cannot Be Made Up

**Hard deadline rationale**:
- **Collaborative work**: Others depend on receiving feedback
- **Cycle timing**: After deadline, peers have already used (or not used) feedback
- **Individual make-up impossible**: Can't recreate peer review with different reviewers after the fact

**Missed peer review = lose those points** (harsh but necessary):
- One cycle missed = ~1.67% loss (recoverable)
- Two cycles missed = ~3.3% loss (concerning but not catastrophic)
- Three+ cycles missed = signals deeper engagement problem

**Alternative approach rejected**: "Make up by writing extra reflective one-pager"
- Problem: Conflates giving feedback with reflection on feedback
- Undermines collaborative nature of peer review
- Would require manufacturing fake peer comments for student to reflect on

---

## Equity Considerations

### Accessibility of Completion-Based Grading

**Advantages for diverse learners**:
- **ESL students**: Not penalized for language differences if ideas are genuine
- **Different dialects**: Labor valued regardless of code-switching ability
- **Writing anxiety**: Reduced pressure allows more authentic writing
- **Processing differences**: Messy thinking acceptable

**Disadvantages to mitigate**:
- **Self-directed learners**: Might prefer quality assessment (portfolio provides this)
- **High achievers**: May see completion as "dumbing down" (quality baseline + portfolio address)
- **Perfectionists**: Might over-polish anyway (encourage risk-taking explicitly)

**Built-in flexibility**:
- Lowest 10 quizzes dropped (illness, caregiving, emergencies)
- 12-15 dev logs (not rigid 15 weekly)
- Cumulative fragment total (can catch up after difficult week)

---

## Evidence & Iteration

### Evidence to Collect

**System effectiveness**:
- [ ] Fragment accumulation patterns (do students hit 40-50 target?)
- [ ] Helpfulness rating distribution (average >3.5?)
- [ ] Reflective one-pager quality (do they cite specific feedback?)
- [ ] Dev log substantiveness (genuine reflection or perfunctory?)
- [ ] Quiz performance (do drops get used strategically?)

**Grade distribution**:
- [ ] Formative grades centering at 45-49% for engaged students?
- [ ] Correlation between formative engagement and portfolio quality?
- [ ] Do students gaming system earn lower overall grades?

**Student experience**:
- [ ] Mid-semester survey: Does grading reduce or increase anxiety?
- [ ] Do students understand difference between completion-based formative and quality-based portfolio?

### Revision Notes

Based on piloting, consider:
- **Fragment quality baseline**: If 3% doesn't prevent gaming, increase to 5%
- **Helpfulness target**: If >3.5 too easy or too hard, adjust
- **Reflective one-pagers**: If students struggle, provide more models
- **Quiz drops**: If 10 too generous or too stingy, adjust to 8 or 12

---

## Scholarly Sources

### Direct Citations

**Labor-based grading**:
- Inoue, Asao B. *Labor-Based Grading Contracts: Building Equity and Inclusion in the Compassionate Writing Classroom*. WAC Clearinghouse, 2019.
- Elbow, Peter. "Ranking, Evaluating, and Liking: Sorting Out Three Forms of Judgment." *College English* 55.2 (1993): 187-206.

**Formative assessment**:
- Bean, John C. *Engaging Ideas: The Professor's Guide to Integrating Writing, Critical Thinking, and Active Learning in the Classroom*. 2nd ed. Jossey-Bass, 2011.
- Nicol, David. "From Monologue to Dialogue: Improving Written Feedback Processes in Mass Higher Education." *Assessment & Evaluation in Higher Education* 35.5 (2010): 501-517.

**Peer review quality**:
- Cho, Kwangsu, and Charles MacArthur. "Student Revision with Peer and Expert Reviewing." *Learning and Instruction* 20.4 (2010): 328-338.
- Topping, Keith. "Peer Assessment Between Students in Colleges and Universities." *Review of Educational Research* 68.3 (1998): 249-276.

**Reflection**:
- Yancey, Kathleen Blake. *Reflection in the Writing Classroom*. Utah State UP, 1998.
- Driscoll, Dana Lynn, and Jennifer Wells. "Beyond Knowledge and Skills: Writing Transfer and the Role of Student Dispositions." *Composition Forum* 26 (2012).

**Retrieval practice**:
- Roediger, Henry L., and Jeffrey D. Karpicke. "Test-Enhanced Learning: Taking Memory Tests Improves Long-Term Retention." *Psychological Science* 17.3 (2006): 249-255.
- Agarwal, Pooja K., et al. "Classroom-Based Programs of Retrieval Practice Reduce Middle School and High School Students' Test Anxiety." *Journal of Applied Research in Memory and Cognition* 3.3 (2014): 131-139.

### Background Reading

- Dewey, John. *How We Think: A Restatement of the Relation of Reflective Thinking to the Educative Process*. D.C. Heath, 1933.
- Elbow, Peter. *Writing Without Teachers*. Oxford UP, 1973.
- Moon, Jennifer A. *Reflection in Learning and Professional Development: Theory and Practice*. Kogan Page, 1999.
- Schön, Donald. *The Reflective Practitioner: How Professionals Think in Action*. Basic Books, 1983.
- Sommers, Nancy. "Revision Strategies of Student Writers and Experienced Adult Writers." *College Composition and Communication* 31.4 (1980): 378-388.
- Sommers, Nancy. "Across the Drafts." *College Composition and Communication* 58.2 (2006): 248-257.

---

## Related Justifications

```dataview
TABLE component, identifier, status
FROM "600_teaching/virens_101"
WHERE type = "teaching-course-justification"
  AND contains(tags, "formative") OR contains(tags, "grading")
  AND file.name != this.file.name
SORT component ASC, identifier ASC
```

---

## Paired Content

**See**: [[formative-grading-content-v101]]

---

*Created: 2026-01-21*
*Component: Assessment*
*Course: VIRENS 101*
---
created: 2026-01-23
type: teaching-course-justification
course: virens-101
component: f_assignments
track: justification
identifier: peer-review-protocol-assignment
paired-content: "[[peer-review-protocol-assignment-course-v101]]"
status: draft
tags: [teaching, virens-101, justification, pedagogy, peer-review, eli-review]

# === SCHOLARLY GROUNDING ===
cites-scholars: [Bruffee, Flower, Sommers, Beaufort, Inoue]
primary-theorist: Bruffee

# === DEPENDENCY MANAGEMENT ===
sync-group: [peer-review-specs]
depends-on: []
affects: []
uses-defs: []
last-sync-check: 2026-01-23
attention-flag: ""
---

# Peer Review Protocol: Pedagogical Rationale

> [!info] Justification
> **Component**: Core Assignment (Continuous)
> **Track**: Justification (why this works)
> **Paired with**: [[peer-review-protocol-assignment-course-v101|Course Content]]
> **Status**: draft

---

## Design Rationale

Peer review in VIRENS 101 implements six structured cycles across the semester, each worth 40 points (240 total, lowest dropped = 200 counted). This design addresses three key problems with traditional peer review practices in composition courses:

1. **Generic feedback problem**: Students often provide vague responses ("good job," "add more detail") that don't help writers revise effectively because the feedback lacks specificity and connection to criteria.

2. **One-shot review limitation**: Many courses use peer review sporadically—perhaps once or twice per semester—which doesn't give students enough practice to develop reviewing skills or understand how different kinds of writing need different kinds of feedback.

3. **Feedback-as-endpoint fallacy**: Traditional peer review often treats receiving feedback as the end of the process, without requiring students to actually process, prioritize, and use that feedback strategically.

The six-cycle structure with two protocols (Generous SPQ and Evaluative DES) and required revision plans addresses these problems systematically. Students practice reviewing frequently enough to develop skill, use protocols matched to writing contexts, and must demonstrate strategic use of feedback through revision planning.

---

## Theoretical Grounding

### Composition Pedagogy

**Kenneth Bruffee's collaborative learning theory** provides the foundational rationale for peer review in composition. In "Collaborative Learning and the 'Conversation of Mankind'" (1984), Bruffee argues that knowledge-making is fundamentally social—we learn to write by participating in communities of writers who share conventions, expectations, and values. Peer review operationalizes this theory: students don't just write for an instructor's evaluation; they write for peers who represent the academic community they're joining.

Bruffee emphasizes that collaborative learning works when it's structured rather than spontaneous. Simply telling students "review each other's work" doesn't create productive collaboration. The six cycles with explicit protocols and criteria create the structure Bruffee describes as necessary—students know what to attend to, how to respond, and why their feedback matters. The anonymization through Eli Review also implements Bruffee's insight that collaboration works better when status differences are minimized; students focus on the text rather than on who wrote it.

**Linda Flower's work on reader-based versus writer-based prose** ("Writer-Based Prose: A Cognitive Basis for Problems in Writing," 1979) explains why peer review develops rhetorical awareness. Flower demonstrates that novice writers often produce "writer-based" prose that makes sense to them but confuses readers because it follows the writer's associative thought process rather than readers' needs for explicit connection and context. Peer review makes this distinction visible: when three peers all note confusion at the same paragraph, the writer recognizes that what seemed clear in their mind isn't clear on the page. This recognition drives revision more effectively than instructor commentary alone because it comes from genuine reader response rather than authoritative correction.

The two protocols (SPQ and DES) implement Flower's insight that different writing tasks need different kinds of reading. Personal narrative benefits from SPQ (Sayback-Pointing-Questioning) because writers need to know how readers experience their story—what came through, what didn't, what resonated. Analytical writing benefits from DES (Describe-Evaluate-Suggest) because writers need to know whether their analysis meets disciplinary expectations for evidence, reasoning, and organization. Matching protocol to purpose honors Flower's distinction between reading for experience versus reading for assessment.

**Nancy Sommers' research on response to student writing** ("Responding to Student Writing," 1982) demonstrates that effective feedback must be criterion-referenced rather than subjective. Sommers found that vague comments like "awkward" or "unclear" don't help students revise because they don't specify what's wrong or how to fix it. The check-box criteria in each cycle implement Sommers' recommendation: each criterion identifies something specific to notice (e.g., "The evidence provided directly supports the central claim"), and students must write substantive responses explaining what they observed. This structure helps peer reviewers provide the kind of specific, actionable feedback that Sommers identifies as actually useful for revision.

The revision plan requirement—rating feedback, sequencing priorities, articulating approach—addresses Sommers' later work on how students actually use feedback. Students don't need to follow all feedback; they need to process it strategically, deciding what serves their purposes and what doesn't. The revision plan makes this processing visible and teachable rather than leaving it as invisible individual work.

### Collaborative Learning Theory

**Anne Beaufort's framework for transfer** (*College Writing and Beyond*, 2007) explains why repeated peer review cycles support transfer. Beaufort argues that transfer requires varied practice with metacognitive awareness—students must experience similar skills in different contexts and reflect on what's portable across contexts. Six review cycles with different assignments (narrative, analytical, research-based) and different protocols (generous, evaluative) create conditions for recognizing patterns: "I'm providing feedback on organization here like I did in Cycle 2, but this assignment needs different organizational strategies."

The analytics discussions after each cycle build the metacognitive awareness Beaufort identifies as essential for transfer. When the class examines aggregated review data, students see patterns in their collective feedback: "We're all noticing introduction problems, but our suggestions vary—what does that tell us about what introductions need to do?" This meta-level discussion helps students develop portable concepts about feedback and revision applicable beyond VIRENS 101.

### Assessment Theory

**Asao Inoue's work on labor-based grading** (*Labor-Based Grading Contracts*, 2019) informs the assessment structure. Each cycle is worth 40 points, with 25 points for completing substantive reviews and 15 points for submitting a revision plan. This point distribution emphasizes labor over quality—completing the work matters more than being an exceptional reviewer. The lowest cycle score is dropped, which accommodates the learning curve: students' early review work may be weaker as they develop skill, but that doesn't permanently damage their grade.

This labor-based approach addresses equity concerns Inoue identifies: students from different educational backgrounds may have different familiarity with academic peer review conventions. By grading primarily for completion and engagement rather than quality, the system creates space for skill development without penalizing students who need more time to learn reviewing practices. Quality feedback emerges through practice, not through grade pressure.

---

## Implementation Approach

### Six Cycles, Two Protocols

The six cycles distribute across the semester at strategic moments:

**Cycles 1-2 (Weeks 5-6)**: Early M2, using generous protocol (SPQ) for Literacy Narrative and Spec Document. These assignments involve personal reflection and explanatory writing, where reader response is more valuable than evaluative judgment. Starting with generous protocol establishes peer review as generative rather than just critical.

**Cycle 3 (Week 7)**: Late M2, first use of evaluative protocol (DES) for Constraint Audit. By Week 7, students have experienced generous review twice and are ready for more directive feedback. The Constraint Audit's analytical nature suits evaluative protocol—students need to know if their analysis meets criteria, not just how readers experience it.

**Cycles 4-5 (Weeks 9-10)**: Mid M3, alternating between evaluative (BEAM Source Entries) and generous (BEAM Portfolio Framing) protocols. This alternation demonstrates that protocol choice depends on writing task, not just progression through semester. Source entries need criteria-based evaluation; portfolio framing needs reader response about synthesis and coherence.

**Cycle 6 (Week 12)**: Early M4, evaluative protocol for Documented Inquiry. This is the most complex review cycle—eight criteria, advanced research writing, multiple source integration. By Cycle 6, students have practiced both protocols multiple times and can provide sophisticated evaluative feedback.

This sequencing implements constraint sequencing principles: students experience generous feedback first (building confidence and understanding reader response), then encounter evaluative feedback (building criteria-based assessment skills), then alternate between protocols (recognizing protocol as strategic choice matching writing context).

### The 48-Hour Rhythm

Every cycle follows the same timeline:

- **Night before class**: Draft upload (11:59pm)
- **In class**: Review demonstration and start (1 block, 25 minutes)
- **Within 48 hours**: Complete reviews (11:59pm, two days after class)
- **Within 96 hours**: Revision plan submission (11:59pm Friday, four days after class)
- **Following class**: Analytics discussion (1 block, 25 minutes)

This rhythm serves multiple purposes:

**Predictability**: Students know the timeline and can plan accordingly. The consistency across all six cycles reduces cognitive load—students learn the rhythm once and repeat it.

**Realistic asynchronous collaboration**: The 48-hour window for completing reviews simulates workplace collaboration where feedback happens asynchronously with time for thoughtful response. Students can't just dash off quick reactions; they must read carefully and compose considered responses.

**Staged processing**: The revision plan deadline comes 48 hours after reviews are due, giving students time to read all feedback before deciding how to use it. This prevents reactive revision where students implement the first suggestion they see without considering the full set of responses.

**Collective reflection**: The analytics discussion happens after reviews are submitted but while the assignment is still in revision, creating space for metacognitive reflection that informs ongoing work rather than retrospective analysis after everything's done.

### Integration with Eli Review Platform

Eli Review is purpose-built for structured peer feedback and provides affordances that support pedagogical goals:

**Anonymization**: Students see peer drafts without author names, which reduces status effects and encourages focus on the text rather than the writer.

**Criterion-based interface**: Each review cycle loads specific check-box criteria and open-response prompts, preventing generic "good job" feedback and ensuring reviews address course-specific goals.

**Analytics dashboard**: Instructors and students can see aggregated patterns in feedback—which criteria were most commonly marked, where reviewers agreed or disagreed, what issues appeared across multiple drafts. This data enables the analytics discussions that build metacognitive awareness.

**Revision plan integration**: The platform's back-end rating, sequencing, and plan-writing happen in the same interface where students received feedback, creating seamless workflow from receiving to processing feedback.

**Accountability tracking**: Eli Review tracks who completed reviews on time, who submitted revision plans, and who participated in the full cycle. This visibility supports labor-based assessment—completion is verifiable and transparent.

---

## Competing Values & Tradeoffs

### Standardization vs. Flexibility

**Tension**: Using the same timeline for all six cycles creates predictability but doesn't account for varying assignment complexity. Should more complex assignments get longer review windows?

**Resolution**: The 48-hour window is sufficient for most cycles if students start reviews promptly. The in-class demonstration and start time (25 minutes) gives students momentum and ensures they understand criteria before working asynchronously. The dropped lowest score accommodates situations where a student genuinely couldn't complete a cycle despite the standard timeline—illness, family emergency, unexpected workload collision. This grace mechanism provides flexibility without abandoning the consistent rhythm that makes peer review sustainable.

### Quality vs. Quantity of Feedback

**Tension**: Requiring 3-4 reviews per cycle means students spend significant time on peer work. Would deeper engagement with fewer peers be better?

**Resolution**: Three to four reviews per cycle creates triangulation—students see multiple perspectives on their work, which helps them recognize patterns ("all three reviewers noted this issue") versus idiosyncratic responses ("only one person mentioned this"). The variety also exposes students to different approaches to the same assignment, supporting learning through comparison. Fewer reviews (1-2) wouldn't provide enough triangulation; more reviews (5-6) would be unsustainable given the expectation for substantive responses. Three to four balances exposure with feasibility.

### Process Requirements vs. Student Agency

**Tension**: Requiring revision plans might feel controlling—students must demonstrate they've processed feedback rather than just using it however they want.

**Resolution**: The revision plan doesn't require students to follow all feedback; it requires them to articulate their choices about feedback. Students can rate feedback low, decide not to implement suggestions, and explain why in their plan. This requirement makes visible the strategic thinking that experienced writers do invisibly. The goal isn't compliance ("you must revise exactly as peers suggested") but metacognition ("you must think deliberately about what feedback serves your purposes"). The revision plan assessment (15 points) doesn't evaluate whether students made "correct" revision choices—it evaluates whether they engaged thoughtfully with the feedback they received.

---

## Evidence & Iteration

### Evidence to Collect

- [ ] **Review completion rates**: Are students submitting reviews on time across all six cycles, or do completion rates drop in later cycles? (Fatigue is a risk factor in repeated cycles.)
- [ ] **Review substantiality**: Are reviews becoming more specific and criterion-focused over time, or remaining generic? (This tests whether students develop reviewing skill through practice.)
- [ ] **Protocol effectiveness**: Do students provide different kinds of feedback under generous vs. evaluative protocols, or does protocol choice not affect feedback patterns? (This tests whether protocol differentiation works as intended.)
- [ ] **Revision plan quality**: Do revision plans show strategic thinking (prioritizing, selecting, justifying), or mechanical compliance (rating everything high, listing revisions in arbitrary order)? (This reveals whether the metacognitive work is happening.)
- [ ] **Analytics discussion insights**: What patterns emerge in class discussions? Do students recognize shared challenges and transfer insights across cycles?
- [ ] **Student reflections**: End-of-movement reflections and process archaeology can reveal whether students value peer review or see it as bureaucratic requirement.
- [ ] **Grade distribution**: With lowest score dropped and labor-based grading, do most students score high (indicating process is accessible), or are scores widely distributed (indicating process barriers)?

### Revision Notes

**First-pilot questions**:
- Is the 48-hour review window sufficient, or do students need 72 hours for more complex assignments?
- Do students understand the difference between generous and evaluative protocols, or is this distinction confusing?
- Is the revision plan requirement helping students process feedback strategically, or does it feel like busywork?
- Are analytics discussions productive, or would class time be better used for writing?
- Is six cycles the right number, or would four deeper cycles work better?
- Does the dropped lowest score create appropriate flexibility, or do students strategically skip one cycle knowing it won't count?

---

## Scholarly Sources

### Direct Citations

Beaufort, Anne. *College Writing and Beyond: A New Framework for University Writing Instruction*. Utah State UP, 2007.

Bruffee, Kenneth A. "Collaborative Learning and the 'Conversation of Mankind.'" *College English* 46.7 (1984): 635-652.

Flower, Linda. "Writer-Based Prose: A Cognitive Basis for Problems in Writing." *College English* 41.1 (1979): 19-37.

Inoue, Asao B. *Labor-Based Grading Contracts: Building Equity and Inclusion in the Compassionate Writing Classroom*. WAC Clearinghouse, 2019.

Sommers, Nancy. "Responding to Student Writing." *College Composition and Communication* 33.2 (1982): 148-156.

---

## Related Notes

- **Course Content**: [[peer-review-protocol-assignment-course-v101]]
- **Eli Review Implementation**: [[eli-review-student-guide-course-v101]]
- **Assessment Philosophy**: [[grading-assessment-reasons-v101]]
- **Pedagogical Innovations**: [[07_pedagogical_innovations]]
- **Outcomes**: [[outcome-workflow-technologies]], [[outcome-rhetorical-awareness]]
